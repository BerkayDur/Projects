{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b852d4ef-b403-48fd-b6bb-493eb91d6efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5d082df-20ab-4bb8-b862-b3e88244c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d78c726-5dff-4518-93ba-458dccb72663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fdc890a-a6c1-4725-b534-dd969cdbdac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWordsFromFile(filename):\n",
    "    stop_words = []\n",
    "    with open(filename) as file:\n",
    "        for i in csv.reader(file):\n",
    "            stop_words.append(i[0].lower())\n",
    "    return stop_words\n",
    "\n",
    "def StopWordRemoval(text, lexicon):\n",
    "    text = text\n",
    "    for i in lexicon:\n",
    "        while i in text:\n",
    "            text.remove(i)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17a2c8c6-54ef-48c3-bc58-aaf3e026398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FamousPeopleFromFile(filename):\n",
    "    famous_people=[]\n",
    "    with open(filename) as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for i in reader:\n",
    "            famous_people.append(i[0].lower())\n",
    "    return famous_people\n",
    "\n",
    "def FamousPeopleTransform(text,lexicon):\n",
    "    famous_matches = []\n",
    "    text = text\n",
    "    for i in lexicon:\n",
    "        if i in text:\n",
    "            text = re.sub(f\"(^|\\W){i}\", f\" |{len(famous_matches)} \", text)\n",
    "            famous_matches.append(i)\n",
    "    return [text, famous_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73e11210-ef1c-4fe8-b489-5aa1655c79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CaseFold(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1983d5f8-5eab-4b21-9211-910cbe21fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nGram(text,n=[1]):\n",
    "    n = [1] + n\n",
    "    text = text\n",
    "    ngrams = []\n",
    "\n",
    "    n.sort()\n",
    "    for i in range(len(n)):\n",
    "        start = []\n",
    "        end = []\n",
    "        if n[i] == 1:\n",
    "            continue\n",
    "        for v in range(n[i]-n[i-1]):\n",
    "            start.append(\"<s>\")\n",
    "            end.append(\"</s>\")\n",
    "        # start = [\"<s>\" for v in range(i-1-count)]\n",
    "        # end = [\"</s>\" for v in range(i-1-count)]\n",
    "        text = start + text + end\n",
    "        for j in range(len(text)-n[i]+1):\n",
    "            ngrams.append(\" \".join(text[j:j+n[i]]))\n",
    "    return text+ngrams  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfdef761-8733-481a-ad63-ff6a053ee84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegationWordsFromFile(filename):\n",
    "    negation_words = []\n",
    "    with open(filename) as file:\n",
    "        for i in csv.reader(file):\n",
    "            negation_words.append(i[0].lower())\n",
    "    return negation_words\n",
    "\n",
    "def Negate(text, lexicon):\n",
    "    text = text\n",
    "    negation_words = lexicon\n",
    "    for i in negation_words:\n",
    "        if i in text:\n",
    "            matches=[]\n",
    "            for k in range(len(text)):\n",
    "                if text[k] == i:\n",
    "                    matches.append(k)\n",
    "            for m in matches:\n",
    "                for j in range(m+1, len(text)):\n",
    "                    if (text[j]==\"not\" or \"|\" in text[j] ):\n",
    "                        continue\n",
    "                    text[j] = \"not_\" + text[j]\n",
    "                    last = text[j][-1]\n",
    "                    if (text[j][:8]==\"not_not_\"):\n",
    "                        text[j] = text[j][8:]\n",
    "                    if (text[j][-1]==\".\" or text[j][-1]==\"!\" or text[j][-1]==\"?\"):\n",
    "                        break\n",
    "        if text[-1] == \"not_\" or text[-1] == \"\":\n",
    "            text.pop()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed57607c-f66b-4e93-9582-698cf983e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsFromFile(\"stop_words.csv\")\n",
    "famous_people = FamousPeopleFromFile(\"famous_people.csv\")\n",
    "negation_words = NegationWordsFromFile(\"negation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9053c9d-af5f-420c-976d-1a2508da03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path,encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"Id\":  # skip header\n",
    "                continue\n",
    "            (label, text) = parse_data_line(line)\n",
    "            raw_data.append((text, label))\n",
    "\n",
    "def split_and_preprocess_data(percentage,Case_Folding=True,Punctuation=True,Stop_Word_Removal=False,Famous_People=False,nGramList=[1],Negation_Words=False,\n",
    "                              Average_Word_Length=False,Types_Per_Tokens=False,Sentiment=False, Mutual_Information=\"all\"):\n",
    "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
    "    and performs the preprocessing.\"\"\"\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int((percentage * num_samples))\n",
    "    for (text, label) in raw_data[:num_training_samples]:\n",
    "        train_data.append((to_feature_vector(pre_process(text,Case_Folding=Case_Folding, Punctuation=Punctuation, Stop_Word_Removal=Stop_Word_Removal,Famous_People=Famous_People, nGramList=nGramList,\n",
    "                                                         Negation_Words=Negation_Words),\n",
    "                                             Average_Word_Length=Average_Word_Length,Types_Per_Tokens=Types_Per_Tokens,Sentiment=Sentiment,\n",
    "                                             Mutual_Information=Mutual_Information),label))\n",
    "    for (text, label) in raw_data[num_training_samples:]:\n",
    "        test_data.append((to_feature_vector(pre_process(text,Case_Folding=Case_Folding, Punctuation=Punctuation, Stop_Word_Removal=Stop_Word_Removal,Famous_People=Famous_People, nGramList=nGramList,\n",
    "                                                        Negation_Words=Negation_Words),\n",
    "                                             Average_Word_Length=Average_Word_Length,Types_Per_Tokens=Types_Per_Tokens,Sentiment=Sentiment,\n",
    "                                             Mutual_Information=Mutual_Information),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d35b9a8b-8e4f-4371-9714-64c98ca2dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    \"\"\"Converts the multiple classes into two,\n",
    "    making it a binary distinction between fake news and real.\"\"\"\n",
    "    #return label\n",
    "    # Converting the multiclass labels to binary label\n",
    "    labels_map = {\n",
    "        'true': 'REAL',\n",
    "        'mostly-true': 'REAL',\n",
    "        'half-true': 'REAL',\n",
    "        'false': 'FAKE',\n",
    "        'barely-true': 'FAKE',\n",
    "        'pants-fire': 'FAKE'\n",
    "    }\n",
    "    return labels_map[label]\n",
    "\n",
    "\n",
    "def parse_data_line(data_line):\n",
    "    # Should return a tuple of the label as just FAKE or REAL and the statement\n",
    "    # e.g. (label, statement)\n",
    "    return (convert_label(data_line[1]),data_line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7fe006c4-52a4-44bb-9a48-6bfa0ad83fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a string of one statement\n",
    "def pre_process(text,Case_Folding=False,Punctuation=False, Stop_Word_Removal=False,Famous_People=False,nGramList=[1],Negation_Words=False):\n",
    "    ## Using famous_people, stop_words, negation_words defined above\n",
    "    text = text\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    ## Case Folding\n",
    "    if Case_Folding==True:\n",
    "        text = CaseFold(text)\n",
    "    ## Famous People Matching\n",
    "    if Famous_People==True:\n",
    "        text,famous_matches = FamousPeopleTransform(text,famous_people)\n",
    "    ## RegExp Pattern Match\n",
    "    if Punctuation ==True:\n",
    "        pattern = \"[^a-zA-Z0-9?!\\'\\.\\|]+\"\n",
    "    else:\n",
    "        pattern=\"\\W\"\n",
    "    text = re.split(pattern, text)\n",
    "    ## Stop Words\n",
    "    if Stop_Word_Removal == True:\n",
    "        text = StopWordRemoval(text, stop_words)\n",
    "    ## Lemmatization\n",
    "    ## I am using the NLTK Lemmatization as Morphological analysis is very complex\n",
    "    ## Negation Words\n",
    "    if Negation_Words == True:\n",
    "        text = Negate(text, negation_words)\n",
    "    ## Keep Punctuation\n",
    "    text = \" \".join(text)\n",
    "    replace_pattern = \"([\\.\\?\\!])\"\n",
    "    for match in re.finditer(replace_pattern, text):\n",
    "        text = text.replace(match.group(), f\" {match.group()} \")\n",
    "    text = text.split(\" \")\n",
    "    ## Famous Again\n",
    "    if Famous_People==True and famous_matches != None:\n",
    "        for i in range(len(text)):\n",
    "            if len(text[i])>1 and text[i][0] ==\"|\":\n",
    "                text[i] = famous_matches[int(text[i][1])]\n",
    "    ##\n",
    "    while \"\" in text:\n",
    "        text.remove(\"\")\n",
    "    while \"not_\" in text:\n",
    "        text.remove(\"not_\")   \n",
    "    ## n-Gram\n",
    "    text = nGram(text, nGramList)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a0b81d7-192a-4667-b1c0-72d4212bd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageWordLength(tokens):\n",
    "    smn = 0\n",
    "    for i in tokens:\n",
    "        smn+= len(i)\n",
    "    return smn/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d30b137c-85b5-4c3c-9280-30dc503de543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unique_Word_Per_Total_Words(tokens):\n",
    "    unique = len(set(tokens))\n",
    "    total_words = len(tokens)\n",
    "    return unique/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "277a48c0-9da2-4100-8b10-668bd6c1203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentimentLexiconFromFile(filename):\n",
    "    positive_sentiment = []\n",
    "    negative_sentiment = []\n",
    "    with open(filename) as file:\n",
    "        for i in csv.reader(file):\n",
    "            if i[0] == \"positive\":\n",
    "                positive_sentiment.append(i[1].lower())\n",
    "            elif i[0] == \"negative\":\n",
    "                negative_sentiment.append(i[1].lower())\n",
    "    return [negative_sentiment, positive_sentiment]\n",
    "\n",
    "def getSentiment(tokens, negative_lexicon,positive_lexicon):\n",
    "    sentiment = {\"Positive Sentiment\":0, \"Negative Sentiment\":0}\n",
    "    for token in tokens:\n",
    "        if token in negative_lexicon:\n",
    "            sentiment[\"Negative Sentiment\"]+=1\n",
    "        elif token in positive_lexicon:\n",
    "            sentiment[\"Positive Sentiment\"]+=1\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52d11d9e-bd2d-473e-9a70-a73b28f02259",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sentiment,positive_sentiment = SentimentLexiconFromFile(\"sentiment_lexicon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04706072-f6db-4bc6-ab96-afa846341251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6236\n",
      "8878\n",
      "13962\n"
     ]
    }
   ],
   "source": [
    "## Do it again but now with feature selection\n",
    "\n",
    "def MutualInformationFeatures(filename):\n",
    "    features = []\n",
    "    with open(filename) as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for i in reader:\n",
    "            features.append(i[0])\n",
    "    return features\n",
    "\n",
    "MI_FAKE = MutualInformationFeatures(\"Q5_pointwise_mutual_information_FAKE.csv\")\n",
    "MI_REAL = MutualInformationFeatures(\"Q5_pointwise_mutual_information_REAL.csv\")\n",
    "MI_Features = list(set(MI_FAKE + MI_REAL))\n",
    "print(len(MI_FAKE))\n",
    "print(len(MI_REAL))\n",
    "print(len(MI_Features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "649fdb28-8d3b-4129-b9f1-dc76db76ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_dict = {} # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens, Average_Word_Length=False, Types_Per_Tokens=False,Sentiment=False,Mutual_Information=\"all\"):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    \n",
    "    features_dict = {}\n",
    "    ## Average word length\n",
    "    if Average_Word_Length == True:\n",
    "        features_dict[\"Average Word Length\"] = AverageWordLength(tokens)\n",
    "    ## Unique_Words/Total_Words\n",
    "    if Types_Per_Tokens==True:\n",
    "        features_dict[\"Unique_Words/Total_Words\"] = Unique_Word_Per_Total_Words(tokens)\n",
    "    ## Sentiment Lexicon\n",
    "    if Sentiment == True:\n",
    "        features_dict.update(getSentiment(tokens, negative_sentiment,positive_sentiment))\n",
    "    ## Mutual_Information Maker\n",
    "    if Mutual_Information != \"all\":\n",
    "        MI_data = MI_Features[:Mutual_Information]\n",
    "    for token in tokens:\n",
    "        if Mutual_Information != \"all\":\n",
    "            \n",
    "            if not(token in MI_data):\n",
    "                continue\n",
    "        features_dict.setdefault(token, 0)\n",
    "        global_feature_dict.setdefault(token,0)\n",
    "        features_dict[token] += 1\n",
    "        global_feature_dict[token] += 1\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc49cf8d-9151-4077-bd39-79ec72c9e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "\n",
    "\n",
    "def train_classifier(data,SVC_HP={}):\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(**SVC_HP))])\n",
    "    return SklearnClassifier(pipeline).train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb3bad0d-4eef-4f12-9684-a0764c4e349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
    "    return classifier.classify_many(samples)\n",
    "\n",
    "def predict_label_from_raw(sample, classifier):\n",
    "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
    "    return classifier.classify(to_feature_vector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13456b64-86fa-4bb2-afca-9635dbf24656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def cross_validate(dataset, folds,return_results=False,print_results=True, pointwiseMI=False,SVC_HP={}):\n",
    "    results = []\n",
    "    ## \n",
    "    fp = []\n",
    "    fp_dict = {}\n",
    "    fn = []\n",
    "    fn_dict = {}\n",
    "    tp = []\n",
    "    tp_dict = {}\n",
    "    tn = []\n",
    "    tn_dict = {}\n",
    "    \n",
    "    if pointwiseMI ==True:\n",
    "        smoothing_factor = 0\n",
    "        entropy_c = 0\n",
    "        features_with_count = {}\n",
    "        features_real = {}\n",
    "        features_fake = {}\n",
    "        information_gain = {}\n",
    "        mutual_information = {}\n",
    "    ## \n",
    "    ##\n",
    "    cv_results = {\n",
    "               \"FAKE\":{\"precision\":0, \"recall\":0, \"f1-score\":0},\n",
    "               \"REAL\":{\"precision\":0, \"recall\":0, \"f1-score\":0},\n",
    "               \"accuracy\":0,\n",
    "               \"macro avg\":{\"precision\":0, \"recall\":0, \"f1-score\":0},\n",
    "               \"weighted avg\":{\"precision\":0, \"recall\":0, \"f1-score\":0}\n",
    "              }\n",
    "    \n",
    "    results_headers = list(cv_results.keys())\n",
    "    results_sub_headers = list(cv_results[\"FAKE\"].keys())\n",
    "    ##\n",
    "    \n",
    "    fold_size = int(len(dataset)/folds) + 1 \n",
    "    for i in range(0,len(dataset),int(fold_size)):\n",
    "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "        if print_results ==True:\n",
    "            print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
    "        # FILL IN THE METHOD HERE\n",
    "        ##\n",
    "        test = dataset[i:i+fold_size]\n",
    "        test_x = []\n",
    "        test_y = []\n",
    "        for data in test:\n",
    "            test_x.append(data[0])\n",
    "            test_y.append(data[1])\n",
    "        \n",
    "        train = dataset[0:i]\n",
    "        train.extend(dataset[i+fold_size:])\n",
    "        model = train_classifier(train,SVC_HP)\n",
    "        predicted = predict_labels(test_x, model)\n",
    "        report = classification_report(test_y, predicted, output_dict=True)\n",
    "        \n",
    "        ##\n",
    "        ##\n",
    "        \n",
    "        if i == 0:         \n",
    "            if pointwiseMI == True:\n",
    "                ## Counts\n",
    "                real_prob = test_y.count(\"REAL\")/len(test_y)\n",
    "                fake_prob = test_y.count(\"FAKE\")/len(test_y)\n",
    "\n",
    "                for value in range(len(test_y)):\n",
    "                    for k in list(test_x[value].keys()):\n",
    "                        features_with_count.setdefault(k,smoothing_factor)\n",
    "                        features_real.setdefault(k,smoothing_factor)\n",
    "                        features_fake.setdefault(k,smoothing_factor)\n",
    "                        features_with_count[k] += 1\n",
    "                        if test_y[value] == \"REAL\":\n",
    "                            features_real[k] += 1\n",
    "                        elif test_y[value] == \"FAKE\":\n",
    "                            features_fake[k] += 1\n",
    "\n",
    "                feature_count_classes = sum(features_with_count.values())\n",
    "                feature_count_real = sum(features_real.values())\n",
    "                feature_count_fake = sum(features_fake.values())\n",
    "\n",
    "                ##\n",
    "                ## Mutual Information\n",
    "\n",
    "                # These dictionaries contain the mutual information for features that occur in that class. This means that I don't have infinities.\n",
    "                # These infinities arise when when we do MLE and the probability in Pointwise mutual information in the numerator in the log is 0.\n",
    "                # This means that feature hasn't occured in that class and thus it is a good discriminator of between the classes. So these features are the best.\n",
    "                # But using the mutual information results directly with infinities isn't that useful. It doesn't tell me the best features for that particular class.\n",
    "                # So I will print to mr, mf, and mutual_information.\n",
    "                # Most likely, the best features to keep are the ones that only occur in REAL or FAKE but not both.\n",
    "\n",
    "                mr = {}\n",
    "                mf = {}\n",
    "                mr_removed = {}\n",
    "                mf_removed = {}\n",
    "\n",
    "                for feature in list(features_with_count.keys()):\n",
    "                    if features_real[feature]>0:\n",
    "                        mr[feature] = np.log2((features_real[feature]/feature_count_real)/((features_with_count[feature]/feature_count_classes)*(real_prob)))\n",
    "                    if features_fake[feature]>0:\n",
    "                        mf[feature] = np.log2((features_fake[feature]/feature_count_fake)/((features_with_count[feature]/feature_count_classes)*(fake_prob)))\n",
    "                    mutual_information.setdefault(feature,0)\n",
    "                    mutual_information[feature] = fake_prob*(mf[feature] if feature in mf else -np.inf) + real_prob*(mr[feature] if feature in mr else -np.inf)\n",
    "\n",
    "                with open(\"Q5_pointwise_mutual_information_FAKE.csv\", \"w\") as file:\n",
    "                    header = [\"Feature\", \"Mutual Information\"]\n",
    "                    writer = csv.DictWriter(file, fieldnames=header)\n",
    "                    writer.writeheader()\n",
    "                    mf_sorted = {k: v for k, v in sorted(mf.items(), key=lambda item: -item[1])}\n",
    "                    for feature,ig in mf_sorted.items():\n",
    "                        writer.writerow({header[0]:feature, header[1]: ig})\n",
    "\n",
    "                with open(\"Q5_pointwise_mutual_information_REAL.csv\", \"w\") as file:\n",
    "                    header = [\"Feature\", \"Pointwise Mutual Information\"]\n",
    "                    writer = csv.DictWriter(file, fieldnames=header)\n",
    "                    writer.writeheader()\n",
    "                    mr_sorted = {k: v for k, v in sorted(mr.items(), key=lambda item: -item[1])}\n",
    "                    for feature,ig in mr_sorted.items():\n",
    "                        writer.writerow({header[0]:feature, header[1]: ig})\n",
    "\n",
    "                with open(\"Q5_average_pointwise_mutual_information.csv\", \"w\") as file:\n",
    "                    header = [\"Feature\", \"Average Pointwise Mutual Information\"]\n",
    "                    writer = csv.DictWriter(file, fieldnames=header)\n",
    "                    writer.writeheader()\n",
    "                    mutual_information_sorted = {k: v for k, v in sorted(mutual_information.items(), key=lambda item: -item[1])}\n",
    "                    for feature,ig in mutual_information_sorted.items():\n",
    "                        writer.writerow({header[0]:feature, header[1]: ig})\n",
    "    \n",
    "        \n",
    "        ##\n",
    "        ## Appending the results for all\n",
    "        results.append(report)\n",
    "        \n",
    "        for header in results_headers:\n",
    "            if header == \"accuracy\":\n",
    "                cv_results[header] += report[\"accuracy\"]/folds\n",
    "                continue\n",
    "            for sub in results_sub_headers:\n",
    "                cv_results[header][sub] += report[header][sub]/folds\n",
    "    if print_results==True:\n",
    "        for header in results_headers:\n",
    "            if header == \"accuracy\":\n",
    "                print(f\"## {header} ## {cv_results[header]}\")\n",
    "                continue\n",
    "            print(f\"## {header} ##\")\n",
    "            for sub in results_sub_headers:\n",
    "                print(f\"{sub} : {cv_results[header][sub]}\")\n",
    "        ##\n",
    "    return cv_results if return_results==True else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bdcc0925-53b3-4130-a5f1-b1928b6622cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 10241 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 10241 rawData, 8192 trainData, 2049 testData\n",
      "Training Samples: \n",
      "8192\n",
      "Features: \n",
      "127099\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "raw_data = []          # the filtered data from the dataset file\n",
    "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "\n",
    "# references to the data files\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "split_and_preprocess_data(0.8, Case_Folding=True,Punctuation=True,Stop_Word_Removal=True,Famous_People=True,nGramList=[1,3],Negation_Words=True,\n",
    "                              Average_Word_Length=True,Types_Per_Tokens=True,Sentiment=True, Mutual_Information=\"all\")\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "314bf3db-7cc9-4237-97f5-68c028713547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Above I have used all the HyperParameters That I made. I will use this to create\n",
    "## a Mutual Information csv file that contains the Mutual Information between a feature and a\n",
    "## class. I haven't used the average Pointwise Mutual Information as this involves infinities\n",
    "## and depending on the amount of smoothing, the values are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1fbf2355-c421-477f-b46c-9a515dee7037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold start on items 0 - 820\n",
      "Fold start on items 820 - 1640\n",
      "Fold start on items 1640 - 2460\n",
      "Fold start on items 2460 - 3280\n",
      "Fold start on items 3280 - 4100\n",
      "Fold start on items 4100 - 4920\n",
      "Fold start on items 4920 - 5740\n",
      "Fold start on items 5740 - 6560\n",
      "Fold start on items 6560 - 7380\n",
      "Fold start on items 7380 - 8200\n",
      "## FAKE ##\n",
      "precision : 0.5335963244004209\n",
      "recall : 0.4868556052289856\n",
      "f1-score : 0.508753186095522\n",
      "## REAL ##\n",
      "precision : 0.6296829241390114\n",
      "recall : 0.6720805139501188\n",
      "f1-score : 0.6499191279163467\n",
      "## accuracy ## 0.5915811606391926\n",
      "## macro avg ##\n",
      "precision : 0.5816396242697162\n",
      "recall : 0.5794680595895522\n",
      "f1-score : 0.5793361570059344\n",
      "## weighted avg ##\n",
      "precision : 0.5885712651304381\n",
      "recall : 0.5915811606391926\n",
      "f1-score : 0.5888853432144717\n"
     ]
    }
   ],
   "source": [
    "# will work and output overall performance of p, r, f-score when cv implemented\n",
    "cross_validate(train_data,10,return_results=False,print_results=True, pointwiseMI=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eaa2aa1-f499-4a0b-8e32-ac1faa6d95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From This result above, we see that the accuracy is much higher than without preprocessing.\n",
    "## From this, I will now perform feature extraction based on the pointwise mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfbce2da-85c1-401c-b928-8c69b240435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis\n",
    "## Stop word Removal, Famous_People, Casefold, nGram, Negation_Words\n",
    "\n",
    "## AVG Word Length, Unique_words_per_total_words, Sentiment, MutualInformation\n",
    "\n",
    "HyperParameters = {\"Case_Folding\":[False,True],\"Punctuation\":[True,False],\"Stop_Word_Removal\":[True, False], \"Famous_People\":[True, False], \"nGramList\":[[1],[2],[3],[5],[1,2,3,4,5],[2,3,5]],\n",
    "              \"Negation_Words\":[True, False],\n",
    "              \"Average_Word_Length\":[True,False], \"Types_Per_Tokens\":[True,False], \"Sentiment\":[True,False],\"Mutual_Information\":[\"all\",1000,4000],\n",
    "              \"SVC_HP\":[{\"loss\":\"hinge\"},{\"C\":0.2},{\"C\":0.4},{\"C\":0.8},{\"C\":1.6}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1631aad5-42bc-47cf-8a32-8010edad17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "##  there is a lot of Hyperparameters, I will look at them individually and then go from there.\n",
    "\n",
    "# results_data= [{\"hyperparameters\":{},score:}]\n",
    "results_data = []\n",
    "\n",
    "raw_data = []          # the filtered data from the dataset file\n",
    "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "for k in list(HyperParameters.keys()):\n",
    "    print(len(results_data))\n",
    "    for v in HyperParameters[k]:\n",
    "        len(results_data)\n",
    "        raw_data = []\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "\n",
    "        data_file_path = 'fake_news.tsv'\n",
    "        load_data(data_file_path) \n",
    "        if k ==\"SVC_HP\":\n",
    "            split_and_preprocess_data(0.8)\n",
    "        else:\n",
    "            split_and_preprocess_data(0.8,**{k:v})\n",
    "        results_data.append({\"hyperparameters\":{k:v},\"score\":cross_validate(train_data, 10, return_results=True,print_results=False,pointwiseMI=False,SVC_HP=v if k==\"SVC_HP\" else {})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d3ba7ef-a329-424c-ab70-966f95e8707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Q5_Hyperparameter_Analysis.csv\",\"w\") as file:\n",
    "    header = [\"hyperparameter\", \"FAKE precision\",\"FAKE recall\",\"FAKE f1-score\",\n",
    "              \"REAL precision\", \"REAL recall\", \"REAL f1-score\",\n",
    "              \"accuracy\",\"macro avg precision\", \"macro avg recall\", \"macro avg f1-score\",\n",
    "              \"weighted avg precision\", \"weighted avg recall\", \"weighted avg f1-score\"]\n",
    "    writer = csv.DictWriter(file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for i in results_data:\n",
    "        t = {\"hyperparameter\":i[\"hyperparameters\"], \n",
    "             \"FAKE precision\":i[\"score\"][\"FAKE\"][\"precision\"],\n",
    "             \"FAKE recall\":i[\"score\"][\"FAKE\"][\"recall\"],\n",
    "             \"FAKE f1-score\":i[\"score\"][\"FAKE\"][\"f1-score\"],\n",
    "             \"REAL precision\":i[\"score\"][\"REAL\"][\"precision\"],\n",
    "             \"REAL recall\":i[\"score\"][\"REAL\"][\"recall\"],\n",
    "             \"REAL f1-score\":i[\"score\"][\"REAL\"][\"f1-score\"],\n",
    "             \"accuracy\":i[\"score\"][\"accuracy\"],\n",
    "             \"macro avg precision\":i[\"score\"][\"macro avg\"][\"precision\"],\n",
    "             \"macro avg recall\":i[\"score\"][\"macro avg\"][\"recall\"],\n",
    "             \"macro avg f1-score\":i[\"score\"][\"macro avg\"][\"f1-score\"],\n",
    "             \"weighted avg precision\":i[\"score\"][\"weighted avg\"][\"precision\"],\n",
    "             \"weighted avg recall\":i[\"score\"][\"weighted avg\"][\"recall\"],\n",
    "             \"weighted avg f1-score\":i[\"score\"][\"weighted avg\"][\"f1-score\"]\n",
    "            }\n",
    "        writer.writerow(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9199a61e-4d16-4b10-a586-dabf8ca78d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Although I would like to combine the above features in hyperparameter turning, this would simply be too long\n",
    "## (performance) so I have looked at each individually and I will remove features based on that and combine the\n",
    "## most useful features at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61290040-488a-401b-b229-b4ee5e44c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on a simple analysis of Q5_Hyperparameter_Analysis.csv, I have \n",
    "## selected the following model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3689116-77ee-46df-91e1-ee2c6fdae1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold start on items 0 - 820\n",
      "Fold start on items 820 - 1640\n",
      "Fold start on items 1640 - 2460\n",
      "Fold start on items 2460 - 3280\n",
      "Fold start on items 3280 - 4100\n",
      "Fold start on items 4100 - 4920\n",
      "Fold start on items 4920 - 5740\n",
      "Fold start on items 5740 - 6560\n",
      "Fold start on items 6560 - 7380\n",
      "Fold start on items 7380 - 8200\n",
      "## FAKE ##\n",
      "precision : 0.553219165942476\n",
      "recall : 0.5035936221934556\n",
      "f1-score : 0.5269828714848982\n",
      "## REAL ##\n",
      "precision : 0.6423294918195631\n",
      "recall : 0.686782402636217\n",
      "f1-score : 0.6636268819633555\n",
      "## accuracy ## 0.6072149465337018\n",
      "## macro avg ##\n",
      "precision : 0.5977743288810194\n",
      "recall : 0.5951880124148362\n",
      "f1-score : 0.5953048767241269\n",
      "## weighted avg ##\n",
      "precision : 0.6041838747884072\n",
      "recall : 0.6072149465337018\n",
      "f1-score : 0.6045518816619581\n"
     ]
    }
   ],
   "source": [
    "Features = {\"Case_Folding\":True, \"Punctuation\":False,\"Stop_Word_Removal\":False,\n",
    "            \"nGramList\":[5], \"Negation_Words\":False, \"Average_Word_Length\":True,\n",
    "            \"Types_Per_Tokens\":True,\"Sentiment\":True,\"Mutual_Information\":\"all\"}\n",
    "HP = {\"loss\":\"hinge\",\"C\":0.2}\n",
    "\n",
    "raw_data = []\n",
    "train_data = []\n",
    "test_data = []\n",
    "    \n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "split_and_preprocess_data(0.8, **Features)\n",
    "cross_validate(train_data, 10, print_results=True, SVC_HP=HP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
