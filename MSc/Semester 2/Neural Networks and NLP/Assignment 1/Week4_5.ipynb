{"cells":[{"cell_type":"markdown","metadata":{"id":"nZfqzG-psHbd"},"source":["#**Lab 4 and 5: Neural Machine Translation (Extra Guide)**"]},{"cell_type":"markdown","metadata":{"id":"r9GBwu1PsSR3"},"source":["This week and the next, we will build a neural machine translation model based on the sequence-to-sequence (seq2seq) models proposed by Sutskever et al., 2014 and Cho et al., 2014. The seq2seq model is widely used in Machine Translation systems such as Google’s neural machine translation system (GNMT) (Wu et al., 2016).\n","\n","The folder **nmt_lab_files** has been provided for you. This folder contains 3 files:\n","1. **data.30.vi** - a file where each line contains a Vietnamese sentence to be translated (i.e. the source sentences)\n","2. **data.30.en** - a file where each line contains an English sentence corresponding to the Vietnamese sentence in the same line position. (i.e. the target sentences)\n","3. **nmt_model_keras.py** - incomplete code for this lab.\n","\n","The pdf file provided contains an explanation of the code file and a guide on how to complete the code (by doing 3 tasks). Read the pdf file and complete the code as instructed."]},{"cell_type":"markdown","metadata":{"id":"KyDvxbvTt70n"},"source":["##**LanguageDict**\n","\n","LanguageDict is a class for creating language dict objects."]},{"cell_type":"markdown","metadata":{"id":"AtHvI1pGvMBG"},"source":["## **The load_dataset() Method**\n","\n","This helper method reads from the source and target files to load max_num_examples sentences, split those sentences into train, development and test sets, and return relevant data.\n"," \n","As an example of the ouput returned by this code, let's assume we are translating the sentence 'I like rabbits' from English to English (this of course is never the case), such that the tokenised and case-normalised source sentence list and target sentence list are as follows:\n","\n","\n","```\n","# In Vietnamese this would actually be [['tôi', 'thích', 'thỏ']]. \n","# We will use English to English here using the following code.\n","source_words = [['i', 'like', 'rabbits']] \n","target_words = [['i', 'like', 'rabbits']]\n","```\n","The word2ids for the source and target language dictionaries look as follows:\n","```\n","source_dict.word2ids = {'<PAD>': 0, '<UNK>': 1, 'i': 2, 'like': 3, 'rabbits':4}\n","\n","# end and start tokens are added to the target words\n","target_dict.word2ids = {'<PAD>': 0, '<UNK>': 1, '<start>': 2, 'i': 3, 'like': 4, 'rabbits':5, '<end>':6}\n","\n","```\n","Let's also assume that we are training and testing on this dataset of one sentence.\n","The **source words** for train/dev/test will be given as follows:\n","```\n","# [batch_size X max_sent_length]\n","source_words_train = [[2,3,4]] # corresponding to ['i', 'like', 'rabbits']\n","source_words_dev = [[2,3,4]]  # corresponding to ['i', 'like', 'rabbits']\n","source_words_test = [[2,3,4]] # corresponding to ['i', 'like', 'rabbits']\n","```\n","\n","The **target words** for the train data will be given as follows (dev/test do not need target words as the model will generate those):\n","```\n","target_words_train = [[2,3,4,5]] # corresponding to ['<start>', 'i', 'like', 'rabbits']\n","```\n","\n","The **target words labels** for each word will be the next word. The target word labels for train/dev/test data will be given as follows\n","```\n","target_words_train_labels = [[3,4,5,6]] # corresponding to ['i', 'like', 'rabbits', '<end>']\n","target_words_dev_labels = [[3,4,5,6]] # corresponding to ['i', 'like', 'rabbits', '<end>']\n","target_words_test_labels = [[3,4,5,6]] # corresponding to ['i', 'like', 'rabbits', '<end>']\n","```\n","The dimensions for the train target words labels would be expanded to have the following dimentionality:\n","```\n","# [batch_size X max_sent_length array X 1]\n","[[3], [4], [5], [6]]\n","```\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cc8KZaJn3uBK"},"source":["##**Neural Translation Model (NMT)**\n","\n","For NMT, the network (a system of connected layers/models) used for training differs slightly from the network used for inference. Both use the encoder-decoder architecture. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sgtz6XAl4E8D"},"source":["###**Training mode**\n","\n","**Encoder**\n","\n","Given:\n","- `source_words`: a `batch_size(num_sents) x max_sentence_length` array representing the source words. In our mini example, this would be the Vietnamese equivalent of `['i', 'like', 'rabbits']`, i.e. `[['tôi', 'thích', 'thỏ']]`.\n","\n","The following steps comprise the encoder network:\n","\n","1. Transform `source_words` into `source_words_embeddings` using a randomly initialized embedding lookup. `source_words_embeddings` is thus an array with the shape `batch_size(num_sents) x max_sentence_length x embedding_dim`.\n","2. Apply embedding dropout with `embedding_dropout_rate`.\n","3. Use a single `LSTM` with the `hidden_size` units to learn a representation for the source words i.e. to encode the input. \n","\n","    (a.) The hidden and cell states for this `LSTM` are initialized to zeros (i.e. we leave the `initial_state = None` default as is).\n","\n","    (b.) We save the `encoder_outputs` (the sequence not just the last state); and the encoder (hidden and cell) states. \n","\n","This way, the model encodes a representation for the source words. Task 1 guides you to complete the encoder part of the training model.\n","\n","\n","**Decoder (No Attention)**\n","\n","Given:\n","- `target_words`: a `batch_size(i.e. num_sents in batch) x max_sentence_length` array representing the target words. This is a time shifted translation of the source words with an added (prepended) `<START>` token `['<start>', 'i', 'like', 'rabbits']`.\n","\n","The decoding is done in the following steps:\n","\n","1. Transform `target_words` into `target_words_embeddings` using a randomly initialized embedding lookup. `target_words_embeddings` is thus an array with the shape `batch_size x max_sentence_length x embedding_dim`.\n","\n","2. Apply embedding dropout of `embedding_dropout_rate`.\n","\n","3. Use a single `LSTM` with `hidden_size` units to learn a representation for the target words. The context is given to this model by using the encoder states to initialise the decoder LSTM. For example, the encoder state for `'thỏ'` (last word in the input sequence, its hidden representation summarises the sentence) is used to learn the representation for the `'<start>'` token.\n","\n","4. For each token representation, we use a dense layer to output a `target_vocab_size` vector of probabilities to be the next word following the represented token. The output `decoder_outputs_train` is thus an array  with the shape `batch_size x max_sent_length + 1 x target_vocab_size`.\n"]},{"cell_type":"markdown","metadata":{"id":"-4tsMpCYJUk1"},"source":["###**Inference Mode**\n","\n","**Encoder**\n","\n","The inference time encoding follows the same steps as the training time encoding.\n","\n","\n","**Decoder (No attention)**\n","\n","During training time, we passed a `batch_size(num_sents) x max_sentence_length` array representing the target words into the decoder LSTM. The `decoder_lstm` represents the given target sentence using the context from the encoder LSTM (representation for the source sentence).  \n","\n","At test time, several things are different:\n","\n","1. We no longer have access to a complete translation of the source sentence (recall that no `target_words` arrays exist for dev and test sets). Rather we initialise the target words array as follows:\n","\n","    Each expected target sentence contains only a single token index, the index of the `'<start>'` token. So, the target_word_dev/test is a `batch_size x 1` array (see the nmt.eval() function).\n","\n","2. This `batch_size x 1` array is fed to the trained `decoder_lstm` and the predicted array is a `batch_size x 1 x target_vocab_size` such that taking the argmax of this array across the dimension 2 will give the most probable next word. \n","\n","For example, at time_step 0 (first time step) the `step_target_words` is given. It is a `batch_size x 1` array containing the `'<start>'` token. The next word prediction of the decoder is for each sentence (in the batch) the first actual word. \n","\n","\n","At the first time step, the `decoder_lstm` still uses the `encoder_states` as its initial states. At subsequent time steps, it uses its own states from the previous time steps. We hence loop over time steps to generate a new word at a time.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"wO8b5apGhg6j"},"outputs":[],"source":["PATH_TO_FOLDER = \"/home/jovyan/ECS7001p/anotherone/nmt_lab_files\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmSgv44y0TN9","tags":[]},"outputs":[],"source":["SOURCE_PATH = PATH_TO_FOLDER + '/data.30.vi'\n","TARGET_PATH = PATH_TO_FOLDER + '/data.30.en'"]},{"cell_type":"markdown","metadata":{"id":"273YkXga27zD"},"source":["Let's install the Sacrebleu (https://github.com/mjpost/sacrebleu) package for BLEU computation."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"S9s5i45Hhg6l"},"outputs":[],"source":["import os\n","import sys\n","sys.path.insert(0,\"/home/jovyan/ECS7001p/anotherone/nmt_lab_files\")"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"enkRNEvohg6m","outputId":"4e7ff992-e282-4b0f-e6cd-7ea31aa782d6"},"outputs":[{"data":{"text/plain":["['/home/jovyan/ECS7001p/anotherone/nmt_lab_files',\n"," '/home/jovyan/ECS7001p/anotherone',\n"," '/usr/local/etc/jupyter',\n"," '/opt/conda/lib/python310.zip',\n"," '/opt/conda/lib/python3.10',\n"," '/opt/conda/lib/python3.10/lib-dynload',\n"," '',\n"," '/opt/conda/lib/python3.10/site-packages']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["sys.path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6ai1zDFZgMy","tags":[],"outputId":"2229a25a-a537-4839-a68d-8504796dadfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.3.1)\n","Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.22.4)\n","Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.5)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2022.10.31)\n","Requirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.7.0)\n","Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (2.8.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install sacrebleu\n","!pip install keras"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"qCkmspa5hg6o","outputId":"9cb510da-b595-483d-8445-7afb519fc4f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tf in /opt/conda/lib/python3.10/site-packages (1.0.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOTMtc_l0_ut","tags":[]},"outputs":[],"source":["import nmt_model_keras as nmt"]},{"cell_type":"markdown","metadata":{"id":"wFIKOF5sed68"},"source":["##**Training Without Attention**\n","\n","If you have completed Tasks 1 and 2, you are ready to train the NMT model without attention.\n","\n","Run the following cells to train the model for 10 epochs. The model summary is also shown below.\n","\n","If you're using a GPU, training will be no more than 10 minutes and you will get the test BLEU score between 5 and 6. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392122,"status":"ok","timestamp":1675978240784,"user":{"displayName":"julia ive","userId":"11396706016758891847"},"user_tz":0},"id":"IsO7wW6U1w2m","outputId":"2049ac54-91fe-4a98-b292-3db8243ac8ba","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["loading dictionaries\n","read 24000/3000/3000 train/dev/test batches\n","Not using any device.\n","number of tokens in source: 2034, number of tokens in target:2506\n","Task 1(a): Creating the embedding lookups...\n","\n","Task 1(b): Looking up source and target words...\n","\n","Task 1(c): Creating an encoder\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"name":"stderr","output_type":"stream","text":["2023-03-08 13:30:11.023466: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-08 13:30:11.729426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2631 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:e3:00.0, compute capability: 8.6\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","\t\t\t\t\t\t Train Model Summary.\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, None, 100)    203400      ['input_1[0][0]']                \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 100)    250600      ['input_2[0][0]']                \n","                                                                                                  \n"," dropout (Dropout)              (None, None, 100)    0           ['embedding[0][0]']              \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, None, 100)    0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," lstm (LSTM)                    [(None, None, 200),  240800      ['dropout[0][0]']                \n","                                 (None, 200),                                                     \n","                                 (None, 200)]                                                     \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 200),  240800      ['dropout_1[0][0]',              \n","                                 (None, 200),                     'lstm[0][1]',                   \n","                                 (None, 200)]                     'lstm[0][2]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, None, 2506)   503706      ['lstm_1[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,439,306\n","Trainable params: 1,439,306\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","\t\t\t\t\t\t Inference Time Encoder Model Summary.\n","Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding (Embedding)       (None, None, 100)         203400    \n","                                                                 \n"," dropout (Dropout)           (None, None, 100)         0         \n","                                                                 \n"," lstm (LSTM)                 [(None, None, 200),       240800    \n","                              (None, 200),                       \n","                              (None, 200)]                       \n","                                                                 \n","=================================================================\n","Total params: 444,200\n","Trainable params: 444,200\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Putting together the decoder states\n","\t\t\t\t\t\t Decoder Inference Model summary\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 100)    250600      ['input_2[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, None, 100)    0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," input_3 (InputLayer)           [(None, 200)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 200)]        0           []                               \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 200),  240800      ['dropout_1[0][0]',              \n","                                 (None, 200),                     'input_3[0][0]',                \n","                                 (None, 200)]                     'input_4[0][0]']                \n","                                                                                                  \n"," input_5 (InputLayer)           [(None, None, 200)]  0           []                               \n","                                                                                                  \n"," dense (Dense)                  (None, None, 2506)   503706      ['lstm_1[1][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 995,106\n","Trainable params: 995,106\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["None\n","Starting training epoch 1/10\n","  1/240 [..............................] - ETA: 19:26 - loss: 3.7441 - accuracy: 0.0000e+00"]},{"name":"stderr","output_type":"stream","text":["2023-03-08 13:30:17.191796: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"]},{"name":"stdout","output_type":"stream","text":["240/240 [==============================] - 35s 125ms/step - loss: 2.1165 - accuracy: 0.2457\n","Time used for epoch 1: 0 m 34 s\n","Evaluating on dev set after epoch 1/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 2.35\n","Time used for evaluate on dev set: 0 m 11 s\n","Starting training epoch 2/10\n","240/240 [==============================] - 30s 124ms/step - loss: 1.7999 - accuracy: 0.3148\n","Time used for epoch 2: 0 m 29 s\n","Evaluating on dev set after epoch 2/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 3.24\n","Time used for evaluate on dev set: 0 m 9 s\n","Starting training epoch 3/10\n","240/240 [==============================] - 30s 124ms/step - loss: 1.6862 - accuracy: 0.3446\n","Time used for epoch 3: 0 m 29 s\n","Evaluating on dev set after epoch 3/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 4.03\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 4/10\n","240/240 [==============================] - 30s 125ms/step - loss: 1.6044 - accuracy: 0.3631\n","Time used for epoch 4: 0 m 30 s\n","Evaluating on dev set after epoch 4/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 4.59\n","Time used for evaluate on dev set: 0 m 9 s\n","Starting training epoch 5/10\n","240/240 [==============================] - 30s 124ms/step - loss: 1.5465 - accuracy: 0.3750\n","Time used for epoch 5: 0 m 29 s\n","Evaluating on dev set after epoch 5/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 4.90\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 6/10\n","240/240 [==============================] - 30s 125ms/step - loss: 1.5008 - accuracy: 0.3821\n","Time used for epoch 6: 0 m 30 s\n","Evaluating on dev set after epoch 6/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 5.18\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 7/10\n","240/240 [==============================] - 30s 123ms/step - loss: 1.4662 - accuracy: 0.3883\n","Time used for epoch 7: 0 m 29 s\n","Evaluating on dev set after epoch 7/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 5.53\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 8/10\n","240/240 [==============================] - 30s 124ms/step - loss: 1.4371 - accuracy: 0.3923\n","Time used for epoch 8: 0 m 29 s\n","Evaluating on dev set after epoch 8/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 5.66\n","Time used for evaluate on dev set: 0 m 9 s\n","Starting training epoch 9/10\n","240/240 [==============================] - 30s 125ms/step - loss: 1.4123 - accuracy: 0.3964\n","Time used for epoch 9: 0 m 30 s\n","Evaluating on dev set after epoch 9/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 5.72\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 10/10\n","240/240 [==============================] - 30s 123ms/step - loss: 1.3927 - accuracy: 0.3992\n","Time used for epoch 10: 0 m 29 s\n","Evaluating on dev set after epoch 10/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 5.67\n","Time used for evaluate on dev set: 0 m 10 s\n","Training finished!\n","Time used for training: 6 m 46 s\n","Evaluating on test set:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 6.14\n","Time used for evaluate on test set: 0 m 10 s\n"]}],"source":["nmt.main(SOURCE_PATH, TARGET_PATH, use_attention=False)"]},{"cell_type":"markdown","metadata":{"id":"dlMDC3DJi12c"},"source":["##**Training and Decoding with Attention**"]},{"cell_type":"markdown","metadata":{"id":"4cQKwvFqurVY"},"source":["The inputs to the attention layer are encoder and decoder outputs. The attention mechanism:\n","1. Computes a score (Luong's dot product attention score) for each source word\n","2. Weights the encoder representations using these scores.\n","3. Concatenates the weighted encoder representation with the decoder ouput.\n","This new decoder output will now be the input to the `decoder_dense` layer. \n","\n","Step-by-step details for Task 3 are in the pdf file. Once you have completed this Task, you are ready to train with attention. Training time will be no more than 10 minutes using a GPU and you should get a test BLEU score of around 15."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297238,"status":"ok","timestamp":1675977844248,"user":{"displayName":"julia ive","userId":"11396706016758891847"},"user_tz":0},"id":"23t6wfpLkb2F","outputId":"d59f7bab-8a30-45aa-cd90-5da897356649"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading dictionaries\n","read 24000/3000/3000 train/dev/test batches\n","Not using any device.\n","number of tokens in source: 2034, number of tokens in target:2506\n","Task 1(a): Creating the embedding lookups...\n","\n","Task 1(b): Looking up source and target words...\n","\n","Task 1(c): Creating an encoder\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"name":"stderr","output_type":"stream","text":["2023-03-08 13:51:19.600778: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-08 13:51:20.351143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2631 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:e3:00.0, compute capability: 8.6\n"]},{"name":"stdout","output_type":"stream","text":["\t\t\t\t\t\t Train Model Summary.\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, None, 100)    203400      ['input_1[0][0]']                \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," dropout (Dropout)              (None, None, 100)    0           ['embedding[0][0]']              \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 100)    250600      ['input_2[0][0]']                \n","                                                                                                  \n"," lstm (LSTM)                    [(None, None, 200),  240800      ['dropout[0][0]']                \n","                                 (None, 200),                                                     \n","                                 (None, 200)]                                                     \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, None, 100)    0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 200),  240800      ['dropout_1[0][0]',              \n","                                 (None, 200),                     'lstm[0][1]',                   \n","                                 (None, 200)]                     'lstm[0][2]']                   \n","                                                                                                  \n"," attention_layer (AttentionLaye  (None, None, 400)   0           ['lstm[0][0]',                   \n"," r)                                                               'lstm_1[0][0]']                 \n","                                                                                                  \n"," dense (Dense)                  (None, None, 2506)   1004906     ['attention_layer[0][0]']        \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,940,506\n","Trainable params: 1,940,506\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","\t\t\t\t\t\t Inference Time Encoder Model Summary.\n","Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding (Embedding)       (None, None, 100)         203400    \n","                                                                 \n"," dropout (Dropout)           (None, None, 100)         0         \n","                                                                 \n"," lstm (LSTM)                 [(None, None, 200),       240800    \n","                              (None, 200),                       \n","                              (None, 200)]                       \n","                                                                 \n","=================================================================\n","Total params: 444,200\n","Trainable params: 444,200\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Putting together the decoder states\n","\t\t\t\t\t\t Decoder Inference Model summary\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 100)    250600      ['input_2[0][0]']                \n","                                                                                                  \n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":[" dropout_1 (Dropout)            (None, None, 100)    0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," input_3 (InputLayer)           [(None, 200)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 200)]        0           []                               \n","                                                                                                  \n"," input_5 (InputLayer)           [(None, None, 200)]  0           []                               \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 200),  240800      ['dropout_1[0][0]',              \n","                                 (None, 200),                     'input_3[0][0]',                \n","                                 (None, 200)]                     'input_4[0][0]']                \n","                                                                                                  \n"," attention_layer (AttentionLaye  (None, None, 400)   0           ['input_5[0][0]',                \n"," r)                                                               'lstm_1[1][0]']                 \n","                                                                                                  \n"," dense (Dense)                  (None, None, 2506)   1004906     ['attention_layer[1][0]']        \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,496,306\n","Trainable params: 1,496,306\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Starting training epoch 1/10\n","  1/240 [..............................] - ETA: 20:49 - loss: 3.6657 - accuracy: 0.0000e+00"]},{"name":"stderr","output_type":"stream","text":["2023-03-08 13:51:26.233161: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"]},{"name":"stdout","output_type":"stream","text":["240/240 [==============================] - 36s 127ms/step - loss: 2.0490 - accuracy: 0.2707\n","Time used for epoch 1: 0 m 35 s\n","Evaluating on dev set after epoch 1/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 4.74\n","Time used for evaluate on dev set: 0 m 11 s\n","Starting training epoch 2/10\n","240/240 [==============================] - 31s 127ms/step - loss: 1.5758 - accuracy: 0.3969\n","Time used for epoch 2: 0 m 30 s\n","Evaluating on dev set after epoch 2/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 10.27\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 3/10\n","240/240 [==============================] - 30s 126ms/step - loss: 1.3317 - accuracy: 0.4512\n","Time used for epoch 3: 0 m 30 s\n","Evaluating on dev set after epoch 3/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 12.27\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 4/10\n","240/240 [==============================] - 30s 127ms/step - loss: 1.1921 - accuracy: 0.4802\n","Time used for epoch 4: 0 m 30 s\n","Evaluating on dev set after epoch 4/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 13.66\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 5/10\n","240/240 [==============================] - 30s 125ms/step - loss: 1.1032 - accuracy: 0.4999\n","Time used for epoch 5: 0 m 30 s\n","Evaluating on dev set after epoch 5/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 14.63\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 6/10\n","240/240 [==============================] - 30s 127ms/step - loss: 1.0413 - accuracy: 0.5159\n","Time used for epoch 6: 0 m 30 s\n","Evaluating on dev set after epoch 6/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 14.65\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 7/10\n","240/240 [==============================] - 31s 127ms/step - loss: 0.9949 - accuracy: 0.5281\n","Time used for epoch 7: 0 m 30 s\n","Evaluating on dev set after epoch 7/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 14.91\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 8/10\n","240/240 [==============================] - 30s 124ms/step - loss: 0.9598 - accuracy: 0.5366\n","Time used for epoch 8: 0 m 29 s\n","Evaluating on dev set after epoch 8/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 15.08\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 9/10\n","240/240 [==============================] - 31s 128ms/step - loss: 0.9308 - accuracy: 0.5460\n","Time used for epoch 9: 0 m 30 s\n","Evaluating on dev set after epoch 9/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 15.11\n","Time used for evaluate on dev set: 0 m 10 s\n","Starting training epoch 10/10\n","240/240 [==============================] - 30s 127ms/step - loss: 0.9070 - accuracy: 0.5531\n","Time used for epoch 10: 0 m 30 s\n","Evaluating on dev set after epoch 10/10:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 14.65\n","Time used for evaluate on dev set: 0 m 10 s\n","Training finished!\n","Time used for training: 6 m 58 s\n","Evaluating on test set:\n"]},{"name":"stderr","output_type":"stream","text":["That's 100 lines that end in a tokenized period ('.')\n","It looks like you forgot to detokenize your test data, which may hurt your score.\n","If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Model BLEU score: 14.98\n","Time used for evaluate on test set: 0 m 10 s\n"]}],"source":["nmt.main(SOURCE_PATH, TARGET_PATH, use_attention=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUOUAjLYpCMk"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1T0TcxMNRsCYE-sRo72iZkjDcoKaE6vsT","timestamp":1675984528696}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}