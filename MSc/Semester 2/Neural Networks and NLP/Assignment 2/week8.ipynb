{"cells":[{"cell_type":"markdown","metadata":{"id":"nFqAT2TH5EYA"},"source":["#**Lab 8 2022/23: NER**"]},{"cell_type":"markdown","metadata":{"id":"Mx1GOqs55iBH"},"source":["Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lxy43VPR4Kr9"},"outputs":[],"source":["from tensorflow.keras import Input,Model\n","from tensorflow.keras.layers import Dropout,Dense,GRU,Bidirectional\n","import tensorflow as tf\n","import numpy as np\n","import json,time,collections,random\n","from itertools import groupby"]},{"cell_type":"markdown","metadata":{"id":"843qGjmR5vbP"},"source":["Class NERModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81urrGnf5xan"},"outputs":[],"source":["class NERModel(object):\n","  def __init__(self,embedding_path, embedding_size,ner_labels):\n","    self.embedding_path = embedding_path\n","    self.embedding_size = embedding_size\n","    self.embedding_dropout_rate = 0.5\n","    self.hidden_size = 50   ### change this back to 50\n","    self.ffnn_layer = 2    ### change this back to 2\n","    self.hidden_dropout_rate = 0.2\n","    self.embedding_dict = self.load_embeddings()\n","    self.ner_labels = ner_labels\n","    self.ner_labels_mappings = {l:i for i,l in enumerate(ner_labels)}\n","\n","  def load_embeddings(self):\n","    print(\"Loading word embeddings from {}...\".format(self.embedding_path))\n","    embeddings = collections.defaultdict(lambda: np.zeros(self.embedding_size))\n","    for line in open(self.embedding_path):\n","      splitter = line.find(' ')\n","      emb = np.fromstring(line[splitter + 1:], np.float32, sep=' ')\n","      assert len(emb) == self.embedding_size\n","      embeddings[line[:splitter]] = emb\n","    print(\"Finished loading word embeddings\")\n","    return embeddings\n","\n","  def build(self):\n","    word_embeddings = Input(shape=(None,self.embedding_size,))\n","    word_embeddings = Dropout(self.embedding_dropout_rate)(word_embeddings)\n","    \"\"\"\n","    Task 1 Create a two layer Bidirectional GRU and Multi-layer FFNN to compute the ner scores for individual tokens\n","    The shape of the ner_scores is [batch_size, max_sentence_length, number_of_ner_labels]\n","    \"\"\"\n","    gru_1 = Bidirectional(GRU(units=self.hidden_size, recurrent_dropout=self.hidden_dropout_rate,\n","                              return_sequences=True))\n","    gru_2 = Bidirectional(GRU(units=self.hidden_size, recurrent_dropout=self.hidden_dropout_rate,\n","                              return_sequences=True))\n","    \n","    word_output = gru_1(word_embeddings)\n","    word_output = gru_2(word_output)\n","    word_output = Dropout(self.hidden_dropout_rate)(word_output)\n","    ner_scores = Dense(units=self.hidden_size, activation=\"relu\")(word_output)\n","    ner_scores = Dropout(self.hidden_dropout_rate)(ner_scores)\n","        \n","    ner_scores = Dense(units=self.hidden_size, activation=\"relu\")(ner_scores)\n","    ner_scores = Dropout(self.hidden_dropout_rate)(ner_scores)\n","\n","    ner_scores = Dense(units = len(self.ner_labels), activation=\"softmax\")(ner_scores)\n","  \n","\n","    \"\"\"\n","    End Task 1 \n","    \"\"\"\n","    self.model = Model(inputs=[word_embeddings],outputs=ner_scores)\n","    self.model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n","    self.model.summary()\n","\n","\n","\n","  def get_feed_dict_list(self, path,batch_size):\n","    feed_dict_list = []\n","    data_sets = json.loads(open(path).readlines()[0])\n","    sentences = data_sets['sentences']\n","    ners = data_sets['ners']\n","    for i in range(0,len(sentences),batch_size):\n","      batch_start, batch_end = i, min(i+batch_size, len(sentences))\n","      sent_lengths = [len(sent) for sent in sentences[batch_start:batch_end]]\n","      max_sent_length = max(sent_lengths)\n","\n","      word_emb = np.zeros([len(sent_lengths), max_sent_length, self.embedding_size])\n","      for i, sent in enumerate(sentences[batch_start:batch_end]):\n","        for j, word in enumerate(sent):\n","          word_emb[i, j] = self.embedding_dict[word.lower()]\n","\n","      word_ner_labels = np.zeros([len(sent_lengths), max_sent_length])\n","      gold_named_entities = set()\n","      for i, ner in enumerate(ners[batch_start:batch_end]):\n","        for s,e,l in ner:\n","          l_id = self.ner_labels_mappings[l]\n","          gold_named_entities.add((i,s,e,l_id))\n","          for j in range(s,e+1):\n","            word_ner_labels[i,j] = l_id\n","\n","\n","      feed_dict_list.append((\n","        word_emb,\n","        word_ner_labels,\n","        gold_named_entities,\n","        sent_lengths\n","      ))\n","\n","    return feed_dict_list\n","\n","\n","  def batch_generator(self, fd_list):\n","    random.shuffle(fd_list)\n","    for word_embeddings, word_ner_labels, _, _ in fd_list:\n","      yield [word_embeddings], word_ner_labels\n","\n","  def train(self, train_path, dev_path, test_path, epochs,batch_size=100):\n","    train_fd_list = self.get_feed_dict_list(train_path,batch_size)\n","    print(\"Load {} training batches from {}\".format(len(train_fd_list), train_path))\n","\n","    dev_fd_list = self.get_feed_dict_list(dev_path,batch_size)\n","    print(\"Load {} dev batches from {}\".format(len(dev_fd_list), dev_path))\n","\n","    test_fd_list = self.get_feed_dict_list(test_path,batch_size)\n","    print(\"Load {} test batches from {}\".format(len(test_fd_list), test_path))\n","\n","    start_time = time.time()\n","    for epoch in range(epochs):\n","      print(\"\\nStarting training epoch {}/{}\".format(epoch + 1, epochs))\n","      epoch_time = time.time()\n","\n","      self.model.fit(self.batch_generator(train_fd_list), steps_per_epoch=len(train_fd_list))\n","\n","      print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n","      dev_time = time.time()\n","      print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n","      self.eval(dev_fd_list)\n","      print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n","\n","    print(\"\\nTraining finished!\")\n","    print(\"Time used for training: {}\".format(self.time_used(start_time)))\n","\n","    print(\"\\nEvaluating on test set:\")\n","    test_time = time.time()\n","    self.eval(test_fd_list)\n","    print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n","\n","  def eval(self, eval_fd_list):\n","    tp, fn, fp = 0,0,0\n","\n","    tp_check = 0\n","\n","    for word_embeddings, _, gold,sent_lens in eval_fd_list:\n","      predictions = self.model.predict_on_batch([word_embeddings])\n","\n","      \"\"\"\n","      Task 2 create the predictions of NER from the IO label \n","      e.g. \n","      0 I         O\n","      1 met       O \n","      2 John      I-PER \n","      3 this      O\n","      4 afternoon O\n","      should give you a person NE John (x,2,2,1)\n","      where x is the sentence id in the batch, and 2,2 are the start and end indices of the NE,\n","      1 is the id for 'PER'    \n","      \"\"\"\n","\n","      reduced_preds = np.argmax(predictions, axis=2)\n","      pred_ner = set()\n","      for id in range(len(reduced_preds)):\n","        sentence = reduced_preds[id][:sent_lens[id]]\n","        groups = [list(g) for f,g in groupby(sentence)]\n","        start = 0\n","        val = []\n","        for group in groups:\n","          if group[0] == 0:\n","            start += len(group)\n","            continue\n","          pred_ner.add((id, start, start+len(group)-1, group[0]))\n","\n","      tp = len(pred_ner.intersection(gold))\n","      fn = len(pred_ner) - tp\n","      fp = len(gold) - tp\n","\n","      \"\"\"\n","      End Task 2\n","      \"\"\"\n","    print(tp, fp, fn)\n","    p = 0.0 if tp == 0 else tp*1.0/(tp+fp)\n","    r = 0.0 if tp == 0 else tp*1.0/(tp+fn)\n","    f = 0.0 if tp == 0 else 2*p*r/(p+r)\n","    print(\"F1 : {:.2f}%\".format(f * 100))\n","    print(\"Precision: {:.2f}%\".format(p * 100))\n","    print(\"Recall: {:.2f}%\".format(r * 100))\n","\n","  def time_used(self, start_time):\n","    curr_time = time.time()\n","    used_time = curr_time - start_time\n","    m = used_time // 60\n","    s = used_time - 60 * m\n","    return \"%d m %d s\" % (m, s)"]},{"cell_type":"markdown","metadata":{"id":"m6c1Q6eL6MBe"},"source":["**Run**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3674,"status":"ok","timestamp":1681985896124,"user":{"displayName":"Berkay Dur","userId":"11618102433274285496"},"user_tz":-60},"id":"-3z6b0Nu5vu-","outputId":"48a429cd-ca61-4505-fab7-98ab7d746713"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\", force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qZELa9Q6CSg"},"outputs":[],"source":["folder = \"/content/drive/MyDrive/Colab_uni/7001p_ass_2/week8/code/\"\n","# folder = \"/content/drive/MyDrive/7001p_ass_2/week8/code/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bE5jNCvS6OH0","executionInfo":{"status":"ok","timestamp":1681986392959,"user_tz":-60,"elapsed":496839,"user":{"displayName":"Berkay Dur","userId":"11618102433274285496"}},"outputId":"4b4782a3-a265-478c-caf2-fe3d134f0f1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading word embeddings from /content/drive/MyDrive/Colab_uni/7001p_ass_2/week8/code/glove.6B.100d.txt.ner.filtered...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["Finished loading word embeddings\n","Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, None, 100)]       0         \n","                                                                 \n"," bidirectional_2 (Bidirectio  (None, None, 100)        45600     \n"," nal)                                                            \n","                                                                 \n"," bidirectional_3 (Bidirectio  (None, None, 100)        45600     \n"," nal)                                                            \n","                                                                 \n"," dropout_5 (Dropout)         (None, None, 100)         0         \n","                                                                 \n"," dense_3 (Dense)             (None, None, 50)          5050      \n","                                                                 \n"," dropout_6 (Dropout)         (None, None, 50)          0         \n","                                                                 \n"," dense_4 (Dense)             (None, None, 50)          2550      \n","                                                                 \n"," dropout_7 (Dropout)         (None, None, 50)          0         \n","                                                                 \n"," dense_5 (Dense)             (None, None, 5)           255       \n","                                                                 \n","=================================================================\n","Total params: 99,055\n","Trainable params: 99,055\n","Non-trainable params: 0\n","_________________________________________________________________\n","Load 141 training batches from /content/drive/MyDrive/Colab_uni/7001p_ass_2/week8/code/train.conll03.json\n","Load 33 dev batches from /content/drive/MyDrive/Colab_uni/7001p_ass_2/week8/code/dev.conll03.json\n","Load 35 test batches from /content/drive/MyDrive/Colab_uni/7001p_ass_2/week8/code/test.conll03.json\n","\n","Starting training epoch 1/5\n","141/141 [==============================] - 92s 580ms/step - loss: 0.3177 - accuracy: 0.9418\n","Time used for epoch 1: 2 m 30 s\n","Evaluating on dev set after epoch 1/5:\n","13 75 37\n","F1 : 18.84%\n","Precision: 14.77%\n","Recall: 26.00%\n","Time used for evaluate on dev set: 0 m 4 s\n","\n","Starting training epoch 2/5\n","141/141 [==============================] - 81s 572ms/step - loss: 0.0986 - accuracy: 0.9701\n","Time used for epoch 2: 1 m 21 s\n","Evaluating on dev set after epoch 2/5:\n","25 63 61\n","F1 : 28.74%\n","Precision: 28.41%\n","Recall: 29.07%\n","Time used for evaluate on dev set: 0 m 2 s\n","\n","Starting training epoch 3/5\n","141/141 [==============================] - 82s 568ms/step - loss: 0.0690 - accuracy: 0.9789\n","Time used for epoch 3: 1 m 21 s\n","Evaluating on dev set after epoch 3/5:\n","26 62 63\n","F1 : 29.38%\n","Precision: 29.55%\n","Recall: 29.21%\n","Time used for evaluate on dev set: 0 m 2 s\n","\n","Starting training epoch 4/5\n","141/141 [==============================] - 82s 584ms/step - loss: 0.0556 - accuracy: 0.9831\n","Time used for epoch 4: 1 m 22 s\n","Evaluating on dev set after epoch 4/5:\n","28 60 56\n","F1 : 32.56%\n","Precision: 31.82%\n","Recall: 33.33%\n","Time used for evaluate on dev set: 0 m 2 s\n","\n","Starting training epoch 5/5\n","141/141 [==============================] - 82s 584ms/step - loss: 0.0478 - accuracy: 0.9853\n","Time used for epoch 5: 1 m 22 s\n","Evaluating on dev set after epoch 5/5:\n","29 59 52\n","F1 : 34.32%\n","Precision: 32.95%\n","Recall: 35.80%\n","Time used for evaluate on dev set: 0 m 2 s\n","\n","Training finished!\n","Time used for training: 8 m 13 s\n","\n","Evaluating on test set:\n","38 56 57\n","F1 : 40.21%\n","Precision: 40.43%\n","Recall: 40.00%\n","Time used for evaluate on test set: 0 m 2 s\n"]}],"source":["if __name__ == '__main__':\n","  embedding_path = folder + 'glove.6B.100d.txt.ner.filtered'\n","  train_path = folder + 'train.conll03.json'\n","  dev_path =  folder + 'dev.conll03.json'\n","  test_path = folder + 'test.conll03.json'\n","  ner_labels = ['O', 'PER', 'ORG', 'LOC', 'MISC']\n","  embedding_size = 100\n","  model = NERModel(embedding_path,embedding_size, ner_labels)\n","  model.build()\n","  model.train(train_path,dev_path,test_path,5)"]},{"cell_type":"code","source":[],"metadata":{"id":"VqMsapGJPxer"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MqQsT4PsYcoo"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}